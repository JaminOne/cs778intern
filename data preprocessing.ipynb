{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport nltk\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"comments = pd.read_csv('../input/sentiment-analysis-msa-phase-2/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments.text.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments = pd.read_csv('../input/sentiment-analysis-msa-phase-2/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEXT PREPROCESS FUNCTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nstopwords = stopwords.words('english')\nporter_stemmer = PorterStemmer()\nlemmatizer = nltk.WordNetLemmatizer()\n\n\n\n\ndef preprocessing(columntitle):\n    comments.dropna(subset=['text'], inplace=True)\n    columntitle = columntitle.str.lower()\n    columntitle = columntitle.apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)) )\n    columntitle = columntitle.apply(lambda x: ' '.join([word for word in nltk.word_tokenize(x) if word.lower() not in stopwords]))\n    columntitle = columntitle.apply(lambda x: re.sub(r'\\(?http\\S+', '', str(x)))\n    #columntitle = columntitle.apply(lambda x: ' '.join(nltk.word_tokenize(x)))\n    lemmatized = [lemmatizer.lemmatize(word) for word in columntitle]\n    columntitle = columntitle.apply(lambda x: ' '.join(\n    [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)]\n        ) )\n    return columntitle\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments['text'] = preprocessing(comments['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {'neutral': 0, 'negative': -1, 'positive': 1}\n\ncomments = comments.replace({'sentiment': mapping})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {0: 'neutral', -1: 'negative', 1: 'positive'}\n\n#comments['sentiment'].map(mapping) \ncomments = comments.replace({'sentiment': mapping})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Single step from tutorials"},{"metadata":{"trusted":true},"cell_type":"code","source":"#deal with null\ncomments.dropna(subset=['text'], inplace=True)\ncomments.text.isnull().sum()\n#remove punctuation\ncomments.text = comments.text.apply(lambda x: re.sub(r'[^\\w\\s]', '', x) )\n#remove stopwords\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')\ncomments.text = comments.text.apply(lambda x: ' '.join([word for word in nltk.word_tokenize(x) if word.lower() not in stopwords]))\n#Remove links\ncomments.text = comments.text.apply(lambda x: re.sub(r'\\(?http\\S+', '', str(x)))\n#comments.text = comments.text.apply(lambda x: ' '.join(nltk.word_tokenize(x)))\n# Stemming and Lemmatization\nfrom nltk.stem import PorterStemmer\nporter_stemmer = PorterStemmer()\nsample = str(comments['text'])\ntext = nltk.word_tokenize(sample)\nstemmed = [porter_stemmer.stem(word) for word in text]\n#stemmed\nlemmatizer = nltk.WordNetLemmatizer()\nlemmatized = [lemmatizer.lemmatize(word) for word in text]\n#lemmatized\ncomments.text = comments.text.apply(lambda x: ' '.join(\n    [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)]\n        ) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = comments.iloc[:10000,:].text.str.cat(sep='. ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = nltk.word_tokenize(all_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def common_bigrams(tokenized_text, min_freq, top_n):\n    bigram_measures = nltk.collocations.BigramAssocMeasures()\n    finder = BigramCollocationFinder.from_words(tokenized_text)\n    finder.apply_freq_filter(min_freq) \n    finder.nbest(bigram_measures.pmi, top_n)\n    return finder.nbest(bigram_measures.pmi, top_n)\n\nprint(common_bigrams(all_text, 10, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"import wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\nsample = comments.iloc[:10000,:].text.str.cat(sep='. ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(0, 0%%, %d%%)\" % random.randint(1, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstp = STOPWORDS.copy()\nwc = WordCloud(background_color=\"green\", max_words=50,  stopwords=stp)\n# generate word cloud\nwc.generate(sample)\n\nplt.figure(figsize=(10,20))\nplt.imshow(wc.recolor(color_func=grey_color_func, random_state=3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n#FIRST, we initialize VADER so we can use it within our Python script\nsid = SentimentIntensityAnalyzer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments[['neg','neu','pos','compound']] = comments.text.apply(lambda x: pd.Series(sid.polarity_scores(x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training a naive bayes sentiment classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(comments.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, comments['sentiment'],test_size = 0.25, random_state = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\n\n#nb.fit(features_matrix, item_we_want_to_predict)\nnb.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = nb.predict(X_test)\naccuracy_score = metrics.accuracy_score(predicted, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(str('{:04.2f}'.format(accuracy_score*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import ComplementNB\n\n#Fitting the model\nCNB = ComplementNB()\nCNB.fit(X_train, Y_train)\n\n#evaluating the model\n#from sklearn import metrics\naccuracy_score = metrics.accuracy_score(CNB.predict(X_test),Y_test)\n\nprint(str('{:4.2f}'.format(accuracy_score*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nBNB = BernoulliNB()\nBNB.fit(X_train, Y_train)\naccuracy_score_bnb = metrics.accuracy_score(BNB.predict(X_test),Y_test)\nprint('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nX2 = tfidf.fit_transform(comments.text)\n\nGNB = GaussianNB()\n\n\n#splitting the data in test and training\n#from sklearn.model_selection() import train_test_split()\nx_train, x_test, y_train, y_test = train_test_split(X2, comments['sentiment'],test_size=0.25,random_state=5)\n\n#defining the model\n#compilimg the model -> we are going to use already used models GNB, MNB, CNB, BNB\n#fitting the model\n\nCNB.fit(x_train, y_train)\naccuracy_score_cnb = metrics.accuracy_score(CNB.predict(x_test), y_test)\nprint('accuracy_score_cnb = '+str('{:4.2f}'.format(accuracy_score_cnb*100))+'%')\n\nGNB.fit(x_train.todense(), y_train)\naccuracy_score_gnb = metrics.accuracy_score(GNB.predict(x_test.todense()), y_test)\nprint('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To list:\n\nsentences = comments['text'].tolist()\nlabels = comments['sentiment'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sentences[1])\nprint(labels[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_size = int(len(sentences) * 0.8)\n\ntraining_sentences = sentences[0: training_size]\ntesting_senteces = sentences[: training_size]\ntraining_labels = labels[0: training_size]\ntesting_labels = labels[: training_size]\n\n# Put labels into list to use later:\n\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 1000\nembedding_dim = 16\nmax_length = 280\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_senteces)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\nmodel.add(tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(embedding_dim,\n                         return_sequences=True)\n))\nmodel.add(tf.keras.layers.Dense(6, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=False\n)\nnum_epochs=3\nmodelo = model.fit(training_padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final),\n          callbacks=[callbacks])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments.sentiment.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample2 = str(comments['text'])\nwords = nltk.word_tokenize(sample2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments = comments.drop(columns=['textID']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2id = imdb.get_word_index()\nid2word = {i: word for word, i in word2id.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = comments['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nimport tensorflow_datasets as tfds\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = comments.loc[:13740,'text'].values\ny_train = comments.loc[:13740,'sentiment'].values\nX_test = comments.loc[13741:27480,'text'].values\ny_test = comments.loc[13741:27480,'sentiment'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer_obj = Tokenizer()\ntotal_text = X_train + X_test\ntokenizer_obj.fit_on_texts(total_text)\n\n#pad sequences\nmax_length = max ([len(s.split()) for s in total_text])\n\n#define vocabulary size\nvocab_size = len(tokenizer_obj.word_index) + 1\n\nX_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\nX_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n\nX_train_pad = pad_sequences(X_train_tokens, maxlen = max_length, padding = 'post')\nX_test_pad = pad_sequences(X_test_tokens, maxlen = max_length, padding = 'post')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, GRU\nfrom keras.layers.embeddings import Embedding\n\nEMBEDDING_DIM = 128\nlstm_out = 196\n\nprint('Build model...')\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\nmodel.add(LSTM(100))\n#model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n#model.add(tf.keras.layers.Dense(6, activation='relu'))\n#model.add(Dense(1,activation='sigmoid'))\nmodel.add(Dense(2,activation='softmax'))\n\n#model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss = 'categorical_crossentropy', \n              optimizer='adam',\n              metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', \n             optimizer='adam', \n             metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nnum_epochs = 5\n\n\nmodel.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), batch_size=batch_size, epochs=num_epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train word2vec Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"comments.to_csv('MSA_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}